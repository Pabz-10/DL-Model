{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotify Recommendation System - Baseline Model\n",
    "\n",
    "This notebook provides a baseline collaborative filtering model for song recommendations using the Spotify Million Playlist Dataset.\n",
    "\n",
    "**Model:** Matrix Factorization using Truncated SVD (Singular Value Decomposition).\n",
    "**Objective:** Given a set of songs from a user's playlist, recommend new songs they might like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Install and import the necessary libraries. `scikit-learn` provides our SVD implementation, and `pandas` is for data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "**Action Required:** Download the Spotify Million Playlist Dataset and place the `data` directory in the same folder as this notebook or provide the correct path.\n",
    "\n",
    "You can get the data here: [https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge](https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge)\n",
    "\n",
    "We'll load a slice of the data to keep memory usage manageable for this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_playlist_slice(path, slice_num=0):\n",
    "    \"\"\"Loads a single JSON slice file from the dataset.\"\"\"\n",
    "    filename = f'mpd.slice.{slice_num*1000}-{(slice_num+1)*1000-1}.json'\n",
    "    with open(os.path.join(path, filename)) as f:\n",
    "        data = json.load(f)\n",
    "    return data['playlists']\n",
    "\n",
    "# CONFIGURATION: Point this to the 'data' directory of the dataset\n",
    "data_path = './data/' \n",
    "\n",
    "# Load the first 1000 playlists as a sample\n",
    "playlists = load_playlist_slice(data_path, slice_num=0)\n",
    "\n",
    "print(f\"Loaded {len(playlists)} playlists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing & Train-Test Split\n",
    "\n",
    "We will now create two interaction matrices:\n",
    "1. `interaction_matrix_train`: The model will be trained on this. We'll hide a few songs from each playlist.\n",
    "2. `test_set`: This will store the hidden songs for each playlist, which we'll use to evaluate our recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tracks = []\n",
    "for p in playlists:\n",
    "    all_tracks.extend([track['track_uri'] for track in p['tracks']])\n",
    "\n",
    "unique_tracks = sorted(list(set(all_tracks)))\n",
    "track_to_idx = {track: i for i, track in enumerate(unique_tracks)}\n",
    "idx_to_track = {i: track for track, i in track_to_idx.items()}\n",
    "pid_to_idx = {p['pid']: i for i, p in enumerate(playlists)}\n",
    "\n",
    "n_playlists = len(playlists)\n",
    "n_tracks = len(unique_tracks)\n",
    "\n",
    "print(f\"Number of unique playlists (users): {n_playlists}\")\n",
    "print(f\"Number of unique tracks (items): {n_tracks}\")\n",
    "\n",
    "# Create training matrix and test set\n",
    "rows, cols = [], []\n",
    "test_set = {}\n",
    "\n",
    "for p in playlists:\n",
    "    playlist_idx = pid_to_idx[p['pid']]\n",
    "    tracks = [t['track_uri'] for t in p['tracks']]\n",
    "    \n",
    "    # For playlists with enough tracks, hold out some for testing\n",
    "    if len(tracks) > 5:\n",
    "        np.random.shuffle(tracks)\n",
    "        num_holdout = int(len(tracks) * 0.2) # Hold out 20% of tracks\n",
    "        train_tracks = tracks[:-num_holdout]\n",
    "        test_tracks = tracks[-num_holdout:]\n",
    "        test_set[playlist_idx] = test_tracks\n",
    "    else:\n",
    "        train_tracks = tracks\n",
    "        \n",
    "    for track_uri in train_tracks:\n",
    "        if track_uri in track_to_idx:\n",
    "            rows.append(playlist_idx)\n",
    "            cols.append(track_to_idx[track_uri])\n",
    "\n",
    "values = np.ones(len(rows), dtype=np.float32)\n",
    "interaction_matrix_train = csr_matrix((values, (rows, cols)), shape=(n_playlists, n_tracks))\n",
    "\n",
    "print(f\"Training matrix created with shape: {interaction_matrix_train.shape}\")\n",
    "print(f\"Test set created for {len(test_set)} playlists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training (Matrix Factorization)\n",
    "\n",
    "We train the SVD model on the **training matrix** only.\n",
    "\n",
    "**Hyperparameter to Tweak:**\n",
    "- `n_components`: The number of latent factors (embedding size). A larger number can capture more complex patterns but may overfit. Common values are between 50 and 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER\n",
    "N_COMPONENTS = 100 # Embedding size\n",
    "\n",
    "svd = TruncatedSVD(n_components=N_COMPONENTS, random_state=42)\n",
    "\n",
    "# Create playlist (user) embeddings\n",
    "playlist_embeddings = svd.fit_transform(interaction_matrix_train)\n",
    "\n",
    "# Create track (item) embeddings\n",
    "track_embeddings = svd.components_.T\n",
    "\n",
    "print(f\"Playlist embeddings shape: {playlist_embeddings.shape}\")\n",
    "print(f\"Track embeddings shape: {track_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating Recommendations\n",
    "\n",
    "The recommendation logic remains the same, but now we base it on the tracks in the *training* set for a given user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a KNN model on the track embeddings for fast lookups\n",
    "knn = NearestNeighbors(n_neighbors=20, metric='cosine', algorithm='brute')\n",
    "knn.fit(track_embeddings)\n",
    "\n",
    "def recommend_from_playlist(playlist_idx, train_matrix, n_recs=10):\n",
    "    \"\"\"Generates song recommendations for a given playlist index.\"\"\"\n",
    "    \n",
    "    # Get the indices of tracks in the user's training playlist\n",
    "    input_track_indices = train_matrix[playlist_idx].indices\n",
    "    \n",
    "    if len(input_track_indices) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Calculate the average embedding for the input playlist\n",
    "    playlist_vector = np.mean(track_embeddings[input_track_indices], axis=0)\n",
    "    \n",
    "    # Find the nearest neighbors (songs) to this average vector\n",
    "    distances, indices = knn.kneighbors(playlist_vector.reshape(1, -1), n_neighbors=n_recs + len(input_track_indices))\n",
    "    \n",
    "    recommendations = []\n",
    "    for idx in indices.flatten():\n",
    "        # Get the track URI from its index\n",
    "        rec_uri = idx_to_track[idx]\n",
    "        # Add to recommendations if it's not in the original input\n",
    "        if rec_uri not in [idx_to_track[i] for i in input_track_indices]:\n",
    "            recommendations.append(rec_uri)\n",
    "    \n",
    "    return recommendations[:n_recs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Recommendation Function\n",
    "\n",
    "Let's see what the model recommends for a sample playlist from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_playlist_idx = list(test_set.keys())[0]\n",
    "\n",
    "print(f\"Recommendations for playlist index {test_playlist_idx}:\")\n",
    "recs = recommend_from_playlist(test_playlist_idx, interaction_matrix_train, n_recs=10)\n",
    "for i, uri in enumerate(recs):\n",
    "    print(f\"{i+1}. {uri}\")\n",
    "\n",
    "print(\"\n",
    "Held-out songs (ground truth):\")\n",
    "for i, uri in enumerate(test_set[test_playlist_idx]):\n",
    "    print(f\"{i+1}. {uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the Model\n",
    "\n",
    "Now we'll formalize the evaluation using Precision@k and Recall@k. We calculate these metrics for every user in our test set and then average the results.\n",
    "\n",
    "- **Precision@k**: What proportion of our top-k recommendations are relevant (i.e., in the holdout set)?\n",
    "- **Recall@k**: What proportion of the relevant items in the holdout set did we successfully recommend in our top-k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(k, recommendations, holdout_items):\n",
    "    recs_at_k = recommendations[:k]\n",
    "    hits = len(set(recs_at_k) & set(holdout_items))\n",
    "    return hits / k\n",
    "\n",
    "def recall_at_k(k, recommendations, holdout_items):\n",
    "    recs_at_k = recommendations[:k]\n",
    "    hits = len(set(recs_at_k) & set(holdout_items))\n",
    "    return hits / len(holdout_items) if len(holdout_items) > 0 else 0\n",
    "\n",
    "def evaluate_model(k=10):\n",
    "    avg_precision = 0\n",
    "    avg_recall = 0\n",
    "    test_user_count = len(test_set)\n",
    "\n",
    "    for user_idx in tqdm(test_set.keys(), desc=\"Evaluating\"):\n",
    "        recommendations = recommend_from_playlist(user_idx, interaction_matrix_train, n_recs=k)\n",
    "        holdout = test_set[user_idx]\n",
    "        \n",
    "        avg_precision += precision_at_k(k, recommendations, holdout)\n",
    "        avg_recall += recall_at_k(k, recommendations, holdout)\n",
    "\n",
    "    avg_precision /= test_user_count\n",
    "    avg_recall /= test_user_count\n",
    "\n",
    "    print(f\"\n",
    "Evaluation Results (k={k}):\")\n",
    "    print(f\"- Average Precision@{k}: {avg_precision:.4f}\")\n",
    "    print(f\"- Average Recall@{k}: {avg_recall:.4f}\")\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_model(k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps & Tweaking\n",
    "\n",
    "To improve your model, you can now:\n",
    "\n",
    "1.  **Use More Data:** Train on more slices of the dataset and see how the evaluation metrics change.\n",
    "2.  **Tune Hyperparameters:** Change `N_COMPONENTS` and `k` in the evaluation to see their effect on precision and recall. Which `k` gives the best balance?\n",
    "3.  **Try Different Models:** The `implicit` library is a great next step for a more advanced ALS model, which is often better for this type of implicit feedback data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
